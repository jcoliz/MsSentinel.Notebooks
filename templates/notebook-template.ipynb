{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ AI-Assisted Notebook Development\n",
    "\n",
    "This template is designed to help you create Microsoft Sentinel notebooks using an AI assistant.\n",
    "\n",
    "## üìö Reference Documentation\n",
    "\n",
    "Share these links with your AI assistant for context:\n",
    "\n",
    "* [Notebook examples](https://learn.microsoft.com/en-us/azure/sentinel/datalake/notebook-examples)\n",
    "* [Microsoft Sentinel Provider class](https://learn.microsoft.com/en-us/azure/sentinel/datalake/sentinel-provider-class-reference)\n",
    "* [Available system tables](https://learn.microsoft.com/en-us/azure/sentinel/datalake/enable-data-connectors)\n",
    "* [Available workspace tables](https://learn.microsoft.com/en-us/azure/azure-monitor/reference/tables-index)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Your Task Description\n",
    "\n",
    "**Instructions:** In the space below, describe in detail what you want this notebook to accomplish. Be specific about:\n",
    "\n",
    "- **Goal**: What security analysis or operation do you want to perform?\n",
    "- **Data Sources**: Which Sentinel tables do you need to query?\n",
    "- **Analysis**: What metrics, calculations, or transformations are needed?\n",
    "- **Output**: What results do you want to see? (Visualizations, tables, alerts, etc.)\n",
    "- **Use Case**: Who will use this notebook and how?\n",
    "\n",
    "**Example:**\n",
    "> \"I want to analyze failed sign-in attempts from the SigninLogs table over the last 30 days. Calculate the number of failed attempts per user, identify patterns by time of day and location, and flag users with more than 10 failed attempts from different IP addresses. Visualize the results with charts and save high-risk users to a custom table.\"\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úçÔ∏è YOUR DESCRIPTION HERE:\n",
    "\n",
    "*Replace this text with your detailed description of what the notebook should do.*\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ Recommended Workflow\n",
    "\n",
    "Follow this three-phase approach with your AI assistant:\n",
    "\n",
    "### **Phase 1: Design** üìã\n",
    "1. Ask your AI assistant: *\"Based on my description above, create a detailed design document for this notebook.\"*\n",
    "2. Review the design for:\n",
    "   - Data sources and table schemas\n",
    "   - Analysis methodology\n",
    "   - Output schema\n",
    "   - Edge cases and limitations\n",
    "3. Iterate: Provide feedback and refine the design until you're satisfied\n",
    "4. Save the final design in a `DESIGN.md` file\n",
    "\n",
    "### **Phase 2: Implementation Plan** üó∫Ô∏è\n",
    "1. Ask your AI assistant: *\"Create a step-by-step implementation plan with specific cell-by-cell instructions.\"*\n",
    "2. Review the plan for:\n",
    "   - Logical flow and structure\n",
    "   - Required libraries and imports\n",
    "   - Error handling strategies\n",
    "   - Testing approach\n",
    "3. Iterate: Adjust the plan based on your environment and requirements\n",
    "4. Save the final plan in an `IMPLEMENTATION.md` file\n",
    "\n",
    "### **Phase 3: Cell-by-Cell Implementation** üíª\n",
    "1. Ask your AI assistant: *\"Create the first cell according to the implementation plan.\"*\n",
    "2. Review and test each cell:\n",
    "   - Run the cell and verify the output\n",
    "   - Check for errors or unexpected results\n",
    "   - Validate data quality and transformations\n",
    "3. Iterate: If a cell doesn't work as expected:\n",
    "   - Share the error message with your AI assistant\n",
    "   - Describe what you expected vs. what happened\n",
    "   - Ask for corrections or alternative approaches\n",
    "4. Move to the next cell only after the current cell works correctly\n",
    "5. Repeat until the notebook is complete\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Tips for Working with AI\n",
    "\n",
    "**Be Specific:**\n",
    "- Provide exact table names and column names\n",
    "- Specify date ranges and time windows\n",
    "- Define thresholds and scoring criteria clearly\n",
    "\n",
    "**Iterate Incrementally:**\n",
    "- Test each cell before moving to the next\n",
    "- Don't try to build the entire notebook at once\n",
    "- Validate intermediate results\n",
    "\n",
    "**Share Context:**\n",
    "- Show error messages in full\n",
    "- Describe your data characteristics (volume, freshness, etc.)\n",
    "- Explain your security objectives\n",
    "\n",
    "**Ask Questions:**\n",
    "- \"Why did you choose this approach?\"\n",
    "- \"What are the performance implications?\"\n",
    "- \"How can I optimize this query?\"\n",
    "- \"What edge cases should I consider?\"\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Next Steps\n",
    "\n",
    "1. **Fill in your task description** in the section above\n",
    "2. **Share the reference links** with your AI assistant\n",
    "3. **Start with Phase 1**: Request a design document\n",
    "4. **Follow the workflow**: Design ‚Üí Plan ‚Üí Implement\n",
    "5. **Document as you go**: Save DESIGN.md and IMPLEMENTATION.md files\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Example Prompts\n",
    "\n",
    "**For Design Phase:**\n",
    "```\n",
    "Based on my description above and using the Microsoft Sentinel documentation \n",
    "links provided, create a detailed design document for this notebook. Include:\n",
    "- Required tables and their schemas\n",
    "- Analysis methodology\n",
    "- Expected output format\n",
    "- Assumptions and limitations\n",
    "```\n",
    "\n",
    "**For Implementation Phase:**\n",
    "```\n",
    "Create a step-by-step implementation plan for the notebook. Break it down\n",
    "into numbered cells with:\n",
    "- Cell purpose and description\n",
    "- Required PySpark operations\n",
    "- Expected output\n",
    "- Validation checks\n",
    "```\n",
    "\n",
    "**For Code Generation:**\n",
    "```\n",
    "Implement cell #3 from the implementation plan. Include:\n",
    "- Complete working code\n",
    "- Inline comments explaining the logic\n",
    "- Error handling\n",
    "- Print statements showing progress\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to begin? Start by describing your task above, then engage your AI assistant!**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Setup & Configuration\n",
    "\n",
    "*This cell will be created by your AI assistant based on your design and implementation plan.*\n",
    "\n",
    "**Typical contents:**\n",
    "- Import required libraries (PySpark functions, Sentinel provider, etc.)\n",
    "- Initialize Microsoft Sentinel provider\n",
    "- Define configuration parameters (workspace name, time windows, etc.)\n",
    "- Set analysis parameters and thresholds\n",
    "- Display configuration summary\n",
    "\n",
    "---\n",
    "\n",
    "**Ask your AI assistant:** *\"Create cell 1 for setup and configuration according to the implementation plan.\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your AI assistant will generate code here\n",
    "# Example structure:\n",
    "#\n",
    "# from sentinel_lake.providers import MicrosoftSentinelProvider\n",
    "# from pyspark.sql.functions import col, count, when, lit\n",
    "#\n",
    "# WORKSPACE_NAME = \"<YOUR_WORKSPACE_NAME>\"\n",
    "# ANALYSIS_DAYS = 30\n",
    "#\n",
    "# sentinel_provider = MicrosoftSentinelProvider(spark)\n",
    "# print(f\"Configuration loaded: {ANALYSIS_DAYS} day analysis window\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Load Data from Sentinel Tables\n",
    "\n",
    "*This cell loads the required data from Microsoft Sentinel tables.*\n",
    "\n",
    "**Typical contents:**\n",
    "- Read data from one or more Sentinel tables using `sentinel_provider.read_table()`\n",
    "- Apply initial filters (date range, user types, status codes, etc.)\n",
    "- Select relevant columns\n",
    "- Cache the data with `.persist()` for performance\n",
    "- Display row count and sample data\n",
    "\n",
    "**Common tables:**\n",
    "- `SigninLogs` - User authentication events\n",
    "- `AuditLogs` - Administrative actions\n",
    "- `SecurityAlert` - Security alerts\n",
    "- `EntraUsers` - User profile information\n",
    "\n",
    "---\n",
    "\n",
    "**Ask your AI assistant:** *\"Create cell 2 to load data from the required Sentinel tables.\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your AI assistant will generate code here\n",
    "# Example structure:\n",
    "#\n",
    "# print(\"Loading data from SigninLogs...\")\n",
    "# \n",
    "# data_df = (\n",
    "#     sentinel_provider.read_table('SigninLogs', WORKSPACE_NAME)\n",
    "#     .filter(\n",
    "#         (col(\"TimeGenerated\") >= expr(f\"current_timestamp() - INTERVAL {ANALYSIS_DAYS} DAYS\"))\n",
    "#     )\n",
    "#     .select(\"UserId\", \"UserPrincipalName\", \"IPAddress\", \"ResultType\")\n",
    "#     .persist()\n",
    "# )\n",
    "# \n",
    "# print(f\"Loaded {data_df.count()} records\")\n",
    "# data_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cells 3-N: Process and Analyze Data\n",
    "\n",
    "*These cells transform, analyze, and visualize your data.*\n",
    "\n",
    "**Common processing operations:**\n",
    "\n",
    "### Data Transformation\n",
    "- Filter and clean data\n",
    "- Group and aggregate (`.groupBy()`, `.agg()`)\n",
    "- Join multiple data sources\n",
    "- Calculate metrics and scores (`.withColumn()`, `when().otherwise()`)\n",
    "- Create derived fields\n",
    "\n",
    "### Analysis\n",
    "- Statistical calculations (counts, averages, percentiles)\n",
    "- Pattern detection (time-based, location-based, behavioral)\n",
    "- Risk scoring and classification\n",
    "- Anomaly identification\n",
    "- Correlation analysis\n",
    "\n",
    "### Visualization\n",
    "- Bar charts and histograms\n",
    "- Time series plots\n",
    "- Distribution charts\n",
    "- Heatmaps and pivot tables\n",
    "\n",
    "---\n",
    "\n",
    "**Ask your AI assistant:** *\"Create cell 3 to [describe specific operation].\"*\n",
    "\n",
    "**Repeat for each processing step, testing each cell before moving to the next.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing cell 1\n",
    "# Your AI assistant will generate code for the first processing step\n",
    "#\n",
    "# Example: Aggregation\n",
    "# aggregated_df = (\n",
    "#     data_df\n",
    "#     .groupBy(\"UserId\", \"UserPrincipalName\")\n",
    "#     .agg(\n",
    "#         count(\"*\").alias(\"event_count\"),\n",
    "#         countDistinct(\"IPAddress\").alias(\"unique_ips\")\n",
    "#     )\n",
    "# )\n",
    "# aggregated_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing cell 2\n",
    "# Your AI assistant will generate code for the next processing step\n",
    "#\n",
    "# Example: Scoring\n",
    "# scored_df = (\n",
    "#     aggregated_df\n",
    "#     .withColumn(\"risk_score\",\n",
    "#         when(col(\"unique_ips\") > 10, 20)\n",
    "#         .when(col(\"unique_ips\") > 5, 10)\n",
    "#         .otherwise(0)\n",
    "#     )\n",
    "# )\n",
    "# scored_df.orderBy(col(\"risk_score\").desc()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing cell 3 (and more as needed)\n",
    "# Continue adding cells for additional processing steps\n",
    "#\n",
    "# Example: Visualization\n",
    "# import matplotlib.pyplot as plt\n",
    "#\n",
    "# dist_df = scored_df.groupBy(\"risk_level\").count().toPandas()\n",
    "# plt.bar(dist_df[\"risk_level\"], dist_df[\"count\"])\n",
    "# plt.title(\"Risk Level Distribution\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Cell: Save Results\n",
    "\n",
    "*This cell saves your analysis results to a custom Sentinel table.*\n",
    "\n",
    "**Typical contents:**\n",
    "- Prepare final output DataFrame with all required columns\n",
    "- Add metadata columns (TimeGenerated, calculation timestamp, etc.)\n",
    "- Order results appropriately\n",
    "- Write to custom table using `sentinel_provider.save_as_table()`\n",
    "- Handle permission errors gracefully\n",
    "- Provide KQL query examples for accessing the saved data\n",
    "\n",
    "**Custom Table Naming:**\n",
    "- Use suffix `_SPRK` for Spark-generated tables\n",
    "- Example: `FailedSigninAnalysis_SPRK`\n",
    "\n",
    "**Required Permissions:**\n",
    "- Microsoft Sentinel Contributor role, OR\n",
    "- Storage Blob Data Contributor role\n",
    "\n",
    "---\n",
    "\n",
    "**Ask your AI assistant:** *\"Create the final cell to save results to a custom Sentinel table.\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your AI assistant will generate code here\n",
    "# Example structure:\n",
    "#\n",
    "# from pyspark.sql.functions import current_timestamp\n",
    "#\n",
    "# CUSTOM_TABLE_NAME = \"YourAnalysisResults_SPRK\"\n",
    "#\n",
    "# output_df = (\n",
    "#     scored_df\n",
    "#     .withColumn(\"TimeGenerated\", current_timestamp())\n",
    "#     .withColumn(\"calculation_date\", current_timestamp())\n",
    "#     .orderBy(col(\"risk_score\").desc())\n",
    "# )\n",
    "#\n",
    "# try:\n",
    "#     sentinel_provider.save_as_table(\n",
    "#         output_df,\n",
    "#         CUSTOM_TABLE_NAME,\n",
    "#         write_options={\"mode\": \"overwrite\", \"mergeSchema\": \"true\"}\n",
    "#     )\n",
    "#     print(f\"‚úÖ Successfully saved {output_df.count()} records to {CUSTOM_TABLE_NAME}\")\n",
    "#     print(f\"\\nQuery in KQL: {CUSTOM_TABLE_NAME} | take 10\")\n",
    "# except Exception as e:\n",
    "#     print(f\"‚ùå Could not write to table. Results available in output_df variable.\")\n",
    "#     print(f\"Export with: output_df.toPandas().to_csv('results.csv', index=False)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
