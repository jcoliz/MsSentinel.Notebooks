{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Risk Score Simple - Training Notebook\n",
    "\n",
    "This simplified notebook is designed as a **training tool** for users new to Microsoft Sentinel. It demonstrates fundamental concepts:\n",
    "\n",
    "- Reading data from Sentinel tables\n",
    "- Performing basic PySpark aggregations\n",
    "- Calculating simple risk scores\n",
    "- Visualizing results with charts\n",
    "- Writing results back to a custom table\n",
    "\n",
    "## What This Notebook Does\n",
    "\n",
    "Calculates a **50-point user risk score** based on sign-in behavior patterns:\n",
    "- **IP Diversity (20 pts)**: How many different IP addresses?\n",
    "- **Sign-in Frequency (20 pts)**: How many sign-ins in 14 days?\n",
    "- **Consistency Pattern (10 pts)**: Is activity spread out or concentrated?\n",
    "\n",
    "**Risk Levels:** Low (0-15), Medium (16-30), High (31-50)\n",
    "\n",
    "## Data Sources (2 Tables)\n",
    "\n",
    "- **SigninLogs**: User authentication events (UserId, UserPrincipalName, IPAddress)\n",
    "- **EntraUsers**: User profiles (id, displayName, mail, department)\n",
    "\n",
    "## Output\n",
    "\n",
    "Results saved to: `UserRiskScoreSimple_SPRK` (12 columns)\n",
    "\n",
    "## References\n",
    "\n",
    "* [Available workspace tables](https://learn.microsoft.com/en-us/azure/azure-monitor/reference/tables-index)\n",
    "* [Available system tables](https://learn.microsoft.com/en-us/azure/sentinel/datalake/enable-data-connectors)\n",
    "* [Microsoft Sentinel Provider class](https://learn.microsoft.com/en-us/azure/sentinel/datalake/sentinel-provider-class-reference)\n",
    "* [Notebook examples](https://learn.microsoft.com/en-us/azure/sentinel/datalake/notebook-examples)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration\n",
    "\n",
    "In this section, we'll set up our environment and configure parameters for our risk analysis.\n",
    "\n",
    "**What we're doing:**\n",
    "- Import required PySpark libraries for data processing\n",
    "- Initialize the Microsoft Sentinel provider to access data lake tables\n",
    "- Set the analysis time window (14 days)\n",
    "\n",
    "**Expected Output:**\n",
    "Confirmation of configuration settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from sentinel_lake.providers import MicrosoftSentinelProvider\n",
    "from pyspark.sql.functions import (\n",
    "    col, count, countDistinct, when, lit, expr,\n",
    "    current_timestamp, avg, coalesce\n",
    ")\n",
    "\n",
    "# Configuration - Update this with your workspace name\n",
    "WORKSPACE_NAME = \"<YOUR_WORKSPACE_NAME>\"\n",
    "\n",
    "# Analysis window\n",
    "ANALYSIS_DAYS = 14\n",
    "\n",
    "# Initialize Sentinel provider\n",
    "sentinel_provider = MicrosoftSentinelProvider(spark)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"USER RISK SCORE SIMPLE - CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Analysis Window: {ANALYSIS_DAYS} days\")\n",
    "print(f\"Workspace: {WORKSPACE_NAME}\")\n",
    "print(f\"Tables: SigninLogs + EntraUsers\")\n",
    "print(f\"Output: UserRiskScoreSimple_SPRK\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load SigninLogs\n",
    "\n",
    "Load sign-in events for behavioral analysis.\n",
    "\n",
    "**What we're doing:**\n",
    "- Read from the SigninLogs table\n",
    "- Filter for Member users (exclude guests)\n",
    "- Filter for last 14 days only\n",
    "- Select only 3 columns: UserId, UserPrincipalName, IPAddress\n",
    "- Cache the data for faster processing\n",
    "\n",
    "**Key Learning:**\n",
    "- `.read_table()` - How to read from Sentinel tables\n",
    "- `.filter()` - How to filter data with conditions\n",
    "- `.select()` - How to choose specific columns\n",
    "- `.persist()` - How to cache data for reuse\n",
    "\n",
    "**Expected Output:**\n",
    "Count of sign-in events loaded and a sample of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Loading SigninLogs...\")\n",
    "\n",
    "signin_df = (\n",
    "    sentinel_provider.read_table('SigninLogs', WORKSPACE_NAME)\n",
    "    .filter(\n",
    "        (col(\"UserType\") == \"Member\") & \n",
    "        (col(\"UserId\").isNotNull()) &\n",
    "        (col(\"TimeGenerated\") >= expr(f\"current_timestamp() - INTERVAL {ANALYSIS_DAYS} DAYS\"))\n",
    "    )\n",
    "    .select(\"UserId\", \"UserPrincipalName\", \"IPAddress\")\n",
    "    .persist()\n",
    ")\n",
    "\n",
    "signin_count = signin_df.count()\n",
    "print(f\"‚úÖ Loaded {signin_count} sign-in events\")\n",
    "\n",
    "print(\"\\nüìã Sample of sign-in data (first 5 rows):\")\n",
    "signin_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load EntraUsers\n",
    "\n",
    "Load user profile information to enrich our risk scores with context.\n",
    "\n",
    "**What we're doing:**\n",
    "- Read from the EntraUsers table\n",
    "- Filter out null or empty user IDs\n",
    "- Select 4 columns: id, displayName, mail, department\n",
    "- Remove any duplicate user records\n",
    "- Cache the data\n",
    "\n",
    "**Key Learning:**\n",
    "- Working with identity data\n",
    "- `.dropDuplicates()` - Ensuring data quality\n",
    "- Multiple filter conditions with `&`\n",
    "\n",
    "**Expected Output:**\n",
    "Count of user profiles loaded and a sample of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Loading EntraUsers...\")\n",
    "\n",
    "users_df = (\n",
    "    sentinel_provider.read_table('EntraUsers')\n",
    "    .filter(\n",
    "        (col(\"id\").isNotNull()) &\n",
    "        (col(\"id\") != \"\")\n",
    "    )\n",
    "    .select(\"id\", \"displayName\", \"mail\", \"department\")\n",
    "    .dropDuplicates([\"id\"])\n",
    "    .persist()\n",
    ")\n",
    "\n",
    "users_count = users_df.count()\n",
    "print(f\"‚úÖ Loaded {users_count} user profiles\")\n",
    "\n",
    "print(\"\\nüìã Sample of user profile data (first 5 rows):\")\n",
    "users_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calculate Sign-in Metrics\n",
    "\n",
    "Now we aggregate the sign-in data to calculate per-user metrics.\n",
    "\n",
    "**What we're doing:**\n",
    "- Group sign-in events by user (UserId, UserPrincipalName)\n",
    "- Count unique IP addresses per user\n",
    "- Count total sign-ins per user\n",
    "\n",
    "**Key Learning:**\n",
    "- `.groupBy()` - Grouping data for aggregation\n",
    "- `.agg()` - Performing multiple aggregations\n",
    "- `countDistinct()` - Counting unique values\n",
    "- `count(*)` - Counting all rows\n",
    "- `.alias()` - Naming columns\n",
    "\n",
    "**Expected Output:**\n",
    "Top 10 users by sign-in count with their unique IP counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç Calculating sign-in metrics per user...\")\n",
    "\n",
    "signin_metrics = (\n",
    "    signin_df\n",
    "    .groupBy(\"UserId\", \"UserPrincipalName\")\n",
    "    .agg(\n",
    "        countDistinct(\"IPAddress\").alias(\"unique_ip_count\"),\n",
    "        count(\"*\").alias(\"total_signins\")\n",
    "    )\n",
    ")\n",
    "\n",
    "metrics_count = signin_metrics.count()\n",
    "print(f\"‚úÖ Calculated metrics for {metrics_count} users\")\n",
    "\n",
    "print(\"\\nüìä Top 10 Users by Sign-in Count:\")\n",
    "signin_metrics.orderBy(col(\"total_signins\").desc()).show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Join with User Profiles\n",
    "\n",
    "Enrich our sign-in metrics with user profile information.\n",
    "\n",
    "**What we're doing:**\n",
    "- Join signin_metrics with users_df\n",
    "- Use LEFT JOIN to keep all users who signed in\n",
    "- Join on: signin_metrics.UserId = users_df.id\n",
    "- Select columns from both DataFrames using DataFrame.column syntax\n",
    "- Rename displayName to UserDisplayName for clarity\n",
    "\n",
    "**Key Learning:**\n",
    "- LEFT JOIN syntax in PySpark\n",
    "- Qualifying columns from specific DataFrames (DataFrame.column)\n",
    "- Using `.alias()` to rename columns\n",
    "- Why enrichment matters (adds context for analysis)\n",
    "\n",
    "**Expected Output:**\n",
    "Sample of enriched data showing user names and departments alongside metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîó Enriching metrics with user profiles...\")\n",
    "\n",
    "combined_df = (\n",
    "    signin_metrics\n",
    "    .join(\n",
    "        users_df,\n",
    "        signin_metrics.UserId == users_df.id,\n",
    "        \"left\"\n",
    "    )\n",
    "    .select(\n",
    "        col(\"UserId\"),\n",
    "        col(\"UserPrincipalName\"),\n",
    "        users_df.displayName.alias(\"UserDisplayName\"),\n",
    "        users_df.department,\n",
    "        col(\"unique_ip_count\"),\n",
    "        col(\"total_signins\")\n",
    "    )\n",
    ")\n",
    "\n",
    "combined_count = combined_df.count()\n",
    "print(f\"‚úÖ Enriched {combined_count} user records\")\n",
    "\n",
    "print(\"\\nüìã Sample of enriched data (first 5 rows):\")\n",
    "combined_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Calculate Risk Scores\n",
    "\n",
    "Now we calculate risk scores based on the metrics we've gathered.\n",
    "\n",
    "**Risk Categories (50 points total):**\n",
    "\n",
    "1. **IP Diversity Score (0-20 points)**\n",
    "   - 1 IP = 0 (normal single location)\n",
    "   - 2-3 IPs = 5 (home + office or travel)\n",
    "   - 4-6 IPs = 12 (elevated - multiple locations)\n",
    "   - 7+ IPs = 20 (high - suspicious diversity)\n",
    "\n",
    "2. **Frequency Score (0-20 points)**\n",
    "   - ‚â§50 sign-ins = 0 (low activity)\n",
    "   - 51-150 = 5 (moderate activity)\n",
    "   - 151-300 = 12 (high activity)\n",
    "   - 301+ = 20 (very high - unusual)\n",
    "\n",
    "3. **Consistency Score (0-10 points)**\n",
    "   - Detects automation: high frequency from few IPs\n",
    "   - 200+ sign-ins from ‚â§2 IPs = 10 (suspicious)\n",
    "   - 200+ sign-ins from 3+ IPs = 5 (monitor)\n",
    "   - <200 sign-ins = 0 (normal)\n",
    "\n",
    "**Key Learning:**\n",
    "- `.withColumn()` - Creating new columns\n",
    "- `when().otherwise()` - Conditional logic (like IF-THEN-ELSE)\n",
    "- Nested conditions for complex logic\n",
    "- Building composite scores by adding columns\n",
    "- Classification logic (Low/Medium/High)\n",
    "\n",
    "**Expected Output:**\n",
    "Top 10 highest risk users with their risk scores broken down by category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ Calculating risk scores...\")\n",
    "\n",
    "risk_scores = (\n",
    "    combined_df\n",
    "    # IP Diversity Score (0-20)\n",
    "    .withColumn(\"ip_diversity_score\",\n",
    "        when(col(\"unique_ip_count\") == 1, 0)\n",
    "        .when(col(\"unique_ip_count\") <= 3, 5)\n",
    "        .when(col(\"unique_ip_count\") <= 6, 12)\n",
    "        .otherwise(20)\n",
    "    )\n",
    "    # Frequency Score (0-20)\n",
    "    .withColumn(\"frequency_score\",\n",
    "        when(col(\"total_signins\") <= 50, 0)\n",
    "        .when(col(\"total_signins\") <= 150, 5)\n",
    "        .when(col(\"total_signins\") <= 300, 12)\n",
    "        .otherwise(20)\n",
    "    )\n",
    "    # Consistency Score (0-10) - detects automation patterns\n",
    "    .withColumn(\"consistency_score\",\n",
    "        when(col(\"total_signins\") >= 200, \n",
    "            when(col(\"unique_ip_count\") <= 2, 10).otherwise(5)\n",
    "        ).otherwise(0)\n",
    "    )\n",
    "    # Total Risk Score (0-50)\n",
    "    .withColumn(\"total_risk_score\",\n",
    "        col(\"ip_diversity_score\") + \n",
    "        col(\"frequency_score\") + \n",
    "        col(\"consistency_score\")\n",
    "    )\n",
    "    # Risk Level Classification\n",
    "    .withColumn(\"risk_level\",\n",
    "        when(col(\"total_risk_score\") <= 15, \"Low\")\n",
    "        .when(col(\"total_risk_score\") <= 30, \"Medium\")\n",
    "        .otherwise(\"High\")\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Risk scores calculated\")\n",
    "\n",
    "print(\"\\nüî¥ Top 10 Highest Risk Users:\")\n",
    "risk_scores.select(\n",
    "    \"UserPrincipalName\",\n",
    "    \"department\",\n",
    "    \"unique_ip_count\",\n",
    "    \"total_signins\",\n",
    "    \"ip_diversity_score\",\n",
    "    \"frequency_score\",\n",
    "    \"consistency_score\",\n",
    "    \"total_risk_score\",\n",
    "    \"risk_level\"\n",
    ").orderBy(col(\"total_risk_score\").desc()).show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Risk Distribution\n",
    "\n",
    "Create a bar chart showing how many users fall into each risk level.\n",
    "\n",
    "**What we're doing:**\n",
    "- Group users by risk_level and count\n",
    "- Convert PySpark DataFrame to Pandas for plotting\n",
    "- Create a bar chart with matplotlib\n",
    "- Color-code: Green (Low), Orange (Medium), Red (High)\n",
    "- Add percentages on bars\n",
    "\n",
    "**Key Learning:**\n",
    "- `.toPandas()` - Converting Spark to Pandas\n",
    "- Creating bar charts with matplotlib\n",
    "- Color coding for visual clarity\n",
    "- Adding data labels to charts\n",
    "- Professional chart formatting\n",
    "\n",
    "**Expected Output:**\n",
    "A bar chart showing the risk level distribution across all users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Creating risk distribution chart...\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate distribution\n",
    "risk_dist = risk_scores.groupBy(\"risk_level\").count().orderBy(\"risk_level\").toPandas()\n",
    "total_users = risk_scores.count()\n",
    "\n",
    "# Create bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = {\"Low\": \"#2ecc71\", \"Medium\": \"#f39c12\", \"High\": \"#e74c3c\"}\n",
    "bars = plt.bar(\n",
    "    risk_dist[\"risk_level\"], \n",
    "    risk_dist[\"count\"],\n",
    "    color=[colors[level] for level in risk_dist[\"risk_level\"]],\n",
    "    edgecolor=\"black\",\n",
    "    linewidth=1.2\n",
    ")\n",
    "\n",
    "# Add labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width()/2., height,\n",
    "        f\"{int(height)}\\n({int(height)/total_users*100:.1f}%)\",\n",
    "        ha=\"center\", va=\"bottom\", fontsize=12, fontweight=\"bold\"\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"Risk Level\", fontsize=14, fontweight=\"bold\")\n",
    "plt.ylabel(\"Number of Users\", fontsize=14, fontweight=\"bold\")\n",
    "plt.title(f\"User Risk Distribution (Total: {total_users} users)\", \n",
    "          fontsize=16, fontweight=\"bold\", pad=20)\n",
    "plt.grid(axis=\"y\", alpha=0.3, linestyle=\"--\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Chart created\")\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RISK DISTRIBUTION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "for _, row in risk_dist.iterrows():\n",
    "    level = row[\"risk_level\"]\n",
    "    count = row[\"count\"]\n",
    "    pct = count/total_users*100\n",
    "    print(f\"{level:8s}: {count:4d} users ({pct:5.1f}%)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Prepare Final Output\n",
    "\n",
    "Prepare the final DataFrame with all columns properly ordered and metadata added.\n",
    "\n",
    "**What we're doing:**\n",
    "- Add timestamp columns (calculation_date, TimeGenerated)\n",
    "- Select all columns in a logical order:\n",
    "  - Identity fields (UserId, UserPrincipalName, UserDisplayName, department)\n",
    "  - Risk scores (total_risk_score, risk_level, subscores)\n",
    "  - Metrics (unique_ip_count, total_signins)\n",
    "  - Metadata (timestamps)\n",
    "- Order by total_risk_score descending (highest risk first)\n",
    "\n",
    "**Key Learning:**\n",
    "- `current_timestamp()` - Adding timestamps\n",
    "- Column ordering for readability\n",
    "- `.select()` for final schema\n",
    "- `.orderBy()` with descending order\n",
    "\n",
    "**Output Schema (12 columns):**\n",
    "1. UserId\n",
    "2. UserPrincipalName\n",
    "3. UserDisplayName\n",
    "4. department\n",
    "5. total_risk_score\n",
    "6. risk_level\n",
    "7. ip_diversity_score\n",
    "8. frequency_score\n",
    "9. unique_ip_count\n",
    "10. total_signins\n",
    "11. calculation_date\n",
    "12. TimeGenerated\n",
    "\n",
    "**Expected Output:**\n",
    "Sample of the final output showing the top 10 highest risk users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìù Preparing final output...\")\n",
    "\n",
    "output_df = (\n",
    "    risk_scores\n",
    "    .withColumn(\"calculation_date\", current_timestamp())\n",
    "    .withColumn(\"TimeGenerated\", current_timestamp())\n",
    "    .select(\n",
    "        # Identity (4 columns)\n",
    "        \"UserId\",\n",
    "        \"UserPrincipalName\",\n",
    "        \"UserDisplayName\",\n",
    "        \"department\",\n",
    "        # Risk Scores (4 columns)\n",
    "        \"total_risk_score\",\n",
    "        \"risk_level\",\n",
    "        \"ip_diversity_score\",\n",
    "        \"frequency_score\",\n",
    "        # Metrics (2 columns)\n",
    "        \"unique_ip_count\",\n",
    "        \"total_signins\",\n",
    "        # Metadata (2 columns)\n",
    "        \"calculation_date\",\n",
    "        \"TimeGenerated\"\n",
    "    )\n",
    "    .orderBy(col(\"total_risk_score\").desc())\n",
    ")\n",
    "\n",
    "output_count = output_df.count()\n",
    "column_count = len(output_df.columns)\n",
    "\n",
    "print(f\"‚úÖ Output prepared:\")\n",
    "print(f\"   Users: {output_count}\")\n",
    "print(f\"   Columns: {column_count}\")\n",
    "print(f\"   Ordered by: total_risk_score (descending)\")\n",
    "\n",
    "print(\"\\nüìã Sample Final Output (Top 10 highest risk users):\")\n",
    "output_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Write to Custom Table\n",
    "\n",
    "Write the risk scores to a custom Sentinel table for querying and analysis.\n",
    "\n",
    "**Required Permissions:**\n",
    "- **Microsoft Sentinel Contributor** role on the workspace, OR\n",
    "- **Storage Blob Data Contributor** role on the storage account\n",
    "\n",
    "**What we're doing:**\n",
    "- Write output_df to `UserRiskScoreSimple_SPRK` table\n",
    "- Mode: \"overwrite\" - replaces existing data with current analysis\n",
    "- Format: Delta Lake for ACID transactions\n",
    "- Handle permission errors gracefully\n",
    "\n",
    "**Custom Table Benefits:**\n",
    "- Query risk scores directly in KQL\n",
    "- Create workbooks and dashboards\n",
    "- Use in analytics rules for alerting\n",
    "- Join with other Sentinel tables\n",
    "\n",
    "**If write fails:**\n",
    "- Data remains in output_df for analysis\n",
    "- Can export to CSV for manual ingestion\n",
    "- Contact admin for permissions\n",
    "\n",
    "**Expected Output:**\n",
    "Success message with record count, OR permission error with alternatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"üíæ Writing risk scores to custom table...\")\n",
    "print(\"‚ö†Ô∏è  Note: Writing requires Microsoft Sentinel Contributor permissions\\n\")\n",
    "\n",
    "# Custom table name - following the _SPRK convention\n",
    "CUSTOM_TABLE_NAME = \"UserRiskScoreSimple_SPRK\"\n",
    "write_success = False\n",
    "\n",
    "# Save original stderr\n",
    "original_stderr = sys.stderr\n",
    "\n",
    "try:\n",
    "    # Suppress stderr during write attempt to avoid mixed output\n",
    "    sys.stderr = open(os.devnull, 'w')\n",
    "    \n",
    "    # Write using Sentinel provider's save_as_table method\n",
    "    sentinel_provider.save_as_table(\n",
    "        output_df,\n",
    "        CUSTOM_TABLE_NAME,\n",
    "        write_options={\n",
    "            \"mode\": \"overwrite\",\n",
    "            \"mergeSchema\": \"true\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    write_success = True\n",
    "    \n",
    "    # Restore stderr before printing success\n",
    "    sys.stderr = original_stderr\n",
    "    \n",
    "    record_count = output_df.count()\n",
    "    print(f\"‚úÖ Successfully wrote {record_count} records to {CUSTOM_TABLE_NAME}\")\n",
    "    print(f\"\\nüìä Table Details:\")\n",
    "    print(f\"   Table Name: {CUSTOM_TABLE_NAME}\")\n",
    "    print(f\"   Records: {record_count}\")\n",
    "    print(f\"   Columns: {len(output_df.columns)}\")\n",
    "    \n",
    "    print(f\"\\nüîç Sample KQL Queries:\")\n",
    "    print(f\"   // Query all high-risk users\")\n",
    "    print(f\"   {CUSTOM_TABLE_NAME}\")\n",
    "    print(f\"   | where risk_level == 'High'\")\n",
    "    print(f\"   | order by total_risk_score desc\")\n",
    "    print(f\"\")\n",
    "    print(f\"   // Query by department\")\n",
    "    print(f\"   {CUSTOM_TABLE_NAME}\")\n",
    "    print(f\"   | where department == 'IT'\")\n",
    "    print(f\"   | summarize avg(total_risk_score) by risk_level\")\n",
    "    \n",
    "except Exception as e:\n",
    "    # Restore stderr\n",
    "    sys.stderr = original_stderr\n",
    "    \n",
    "    print(f\"‚ùå Could not write to custom table\")\n",
    "    print(f\"\\n‚ö†Ô∏è  Common causes:\")\n",
    "    print(f\"   - Missing Microsoft Sentinel Contributor permissions\")\n",
    "    print(f\"   - Storage account access issues\")\n",
    "    print(f\"   See: https://learn.microsoft.com/en-us/azure/sentinel/roles\")\n",
    "    \n",
    "finally:\n",
    "    # Ensure stderr is always restored\n",
    "    sys.stderr = original_stderr\n",
    "\n",
    "if not write_success:\n",
    "    print(f\"\\nüí° Your risk scores are still available in the 'output_df' variable!\")\n",
    "    print(f\"\\nüìä You can still analyze the data:\")\n",
    "    print(f\"   output_df.filter(col('risk_level') == 'High').show()\")\n",
    "    print(f\"\\nüìÅ To export to CSV:\")\n",
    "    print(f\"   pdf = output_df.toPandas()\")\n",
    "    print(f\"   pdf.to_csv('user_risk_scores_simple.csv', index=False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary & Next Steps\n",
    "\n",
    "## üéâ Congratulations!\n",
    "\n",
    "You've successfully completed your first Sentinel security analytics notebook!\n",
    "\n",
    "### What You Accomplished\n",
    "\n",
    "‚úÖ **Loaded data from 2 Sentinel tables**\n",
    "- SigninLogs (authentication events)\n",
    "- EntraUsers (user profiles)\n",
    "\n",
    "‚úÖ **Performed data operations**\n",
    "- Filtered data with conditions\n",
    "- Aggregated with groupBy() and agg()\n",
    "- Joined two DataFrames\n",
    "- Created calculated columns\n",
    "\n",
    "‚úÖ **Calculated risk scores**\n",
    "- IP diversity scoring (0-20 points)\n",
    "- Sign-in frequency scoring (0-20 points)\n",
    "- Consistency pattern detection (0-10 points)\n",
    "- Risk level classification (Low/Medium/High)\n",
    "\n",
    "‚úÖ **Created visualizations**\n",
    "- Risk distribution bar chart\n",
    "- Color-coded by risk level\n",
    "\n",
    "‚úÖ **Saved results**\n",
    "- Wrote to UserRiskScoreSimple_SPRK table (if permissions allowed)\n",
    "- Created queryable data for KQL\n",
    "\n",
    "---\n",
    "\n",
    "## What You Learned\n",
    "\n",
    "### Sentinel Skills\n",
    "- Connecting to workspace with MicrosoftSentinelProvider\n",
    "- Reading from system tables (SigninLogs, EntraUsers)\n",
    "- Understanding table schemas and relationships\n",
    "- Writing to custom tables for analysis\n",
    "\n",
    "### PySpark Skills\n",
    "- **Filtering**: `.filter()`, `.select()`\n",
    "- **Aggregating**: `.groupBy()`, `.agg()`, `countDistinct()`, `count()`\n",
    "- **Joining**: LEFT JOIN with qualified column names\n",
    "- **Transforming**: `.withColumn()`, `when().otherwise()`\n",
    "- **Sorting**: `.orderBy()` with ascending/descending\n",
    "\n",
    "### Security Analytics Skills\n",
    "- IP diversity as a risk signal\n",
    "- Threshold-based scoring methodology\n",
    "- Risk level classification\n",
    "- Pattern detection (automation, consistency)\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "### 1. Experiment with This Notebook\n",
    "\n",
    "Try modifying the scoring thresholds:\n",
    "```python\n",
    "# In Section 6, change IP diversity thresholds:\n",
    ".withColumn(\"ip_diversity_score\",\n",
    "    when(col(\"unique_ip_count\") == 1, 0)\n",
    "    .when(col(\"unique_ip_count\") <= 5, 5)  # Changed from 3\n",
    "    .when(col(\"unique_ip_count\") <= 10, 12)  # Changed from 6\n",
    "    .otherwise(20)\n",
    ")\n",
    "```\n",
    "\n",
    "Try different time windows:\n",
    "```python\n",
    "# In Section 1:\n",
    "ANALYSIS_DAYS = 7  # Change from 14 to 7 days\n",
    "```\n",
    "\n",
    "### 2. Enhance This Notebook\n",
    "\n",
    "**Add Off-Hours Analysis:**\n",
    "```python\n",
    "# In Section 2, add to .select():\n",
    ".select(\"UserId\", \"UserPrincipalName\", \"IPAddress\", \"TimeGenerated\")\n",
    "\n",
    "# In Section 4, add to .agg():\n",
    "sum(\n",
    "    when((hour(\"TimeGenerated\") < 6) | (hour(\"TimeGenerated\") >= 18), 1)\n",
    "    .otherwise(0)\n",
    ").alias(\"offhours_signins\")\n",
    "```\n",
    "\n",
    "**Add Application Diversity:**\n",
    "```python\n",
    "# In Section 2, add to .select():\n",
    ".select(\"UserId\", \"UserPrincipalName\", \"IPAddress\", \"AppId\")\n",
    "\n",
    "# In Section 4, add to .agg():\n",
    "countDistinct(\"AppId\").alias(\"unique_app_count\")\n",
    "```\n",
    "\n",
    "### 3. Progress to Full Version\n",
    "\n",
    "Once comfortable with this notebook, explore:\n",
    "- **Risk Score.ipynb** - Full production version\n",
    "  - 4 tables (adds AuditLogs, SecurityAlert)\n",
    "  - 6 risk categories (100 points)\n",
    "  - Department baselines\n",
    "  - 27 output columns\n",
    "  - More sophisticated scoring\n",
    "\n",
    "### 4. Apply in Production\n",
    "\n",
    "**Schedule Regular Runs:**\n",
    "- Run daily or weekly to track risk trends\n",
    "- Monitor changes in user risk profiles\n",
    "- Identify emerging threats early\n",
    "\n",
    "**Create Analytics Rules:**\n",
    "```kql\n",
    "UserRiskScoreSimple_SPRK\n",
    "| where risk_level == \"High\"\n",
    "| where total_risk_score >= 40\n",
    "| project UserPrincipalName, total_risk_score, unique_ip_count, department\n",
    "```\n",
    "\n",
    "**Build Workbooks:**\n",
    "- Visualize risk trends over time\n",
    "- Compare departments\n",
    "- Track high-risk users\n",
    "\n",
    "**Join with Other Tables:**\n",
    "```kql\n",
    "UserRiskScoreSimple_SPRK\n",
    "| join kind=inner (\n",
    "    SecurityAlert\n",
    "    | where CompromisedEntity != \"\"\n",
    ") on $left.UserPrincipalName == $right.CompromisedEntity\n",
    "| summarize AlertCount=count() by UserPrincipalName, risk_level\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Resources\n",
    "\n",
    "**Documentation:**\n",
    "- [Sentinel Data Lake Documentation](https://learn.microsoft.com/en-us/azure/sentinel/datalake/)\n",
    "- [PySpark SQL Functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html)\n",
    "- [KQL Reference](https://learn.microsoft.com/en-us/azure/data-explorer/kusto/query/)\n",
    "\n",
    "**Related Notebooks:**\n",
    "- `Risk Score.ipynb` - Full production version\n",
    "- Design documents in `/plans/` directory\n",
    "\n",
    "**Get Help:**\n",
    "- [Microsoft Tech Community - Sentinel](https://techcommunity.microsoft.com/t5/microsoft-sentinel/bd-p/MicrosoftSentinel)\n",
    "- [Stack Overflow - Azure Sentinel](https://stackoverflow.com/questions/tagged/azure-sentinel)\n",
    "\n",
    "---\n",
    "\n",
    "## üéì You're Ready!\n",
    "\n",
    "You now have the foundational skills to:\n",
    "- Work with Sentinel data programmatically\n",
    "- Perform security analytics with PySpark\n",
    "- Calculate risk scores from behavioral data\n",
    "- Create visualizations and reports\n",
    "- Build production security analytics\n",
    "\n",
    "Keep exploring, experimenting, and building!\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medium pool (32 vCores) [User Risk Score Simple]",
   "language": "Python",
   "name": "MSGMedium"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
